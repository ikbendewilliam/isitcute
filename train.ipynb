{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.0 64-bit",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "import glob\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102973440/102967424 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.ResNet50(input_shape=(224, 224, 3), include_top=True, weights=\"imagenet\", input_tensor=None, pooling='max', classes=1000,)\n",
    "base_output = base_model.output\n",
    "final_output = Dense(2, activation='softmax')(base_output)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(dataset):\n",
    "    dataset_path = pathlib.Path('data', dataset)\n",
    "\n",
    "    datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input,\n",
    "                                 rotation_range=40,\n",
    "                                 width_shift_range=0.2,\n",
    "                                 height_shift_range=0.2,\n",
    "                                 zoom_range=0.2,\n",
    "                                 horizontal_flip=True)\n",
    "                                 \n",
    "    return datagen.flow_from_directory(str(dataset_path),\n",
    "                                       target_size=(224, 224),\n",
    "                                       batch_size=32,\n",
    "                                       shuffle=True,\n",
    "                                       class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 10143 images belonging to 2 classes.\n",
      "Found 1509 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 1054s 4s/step - loss: 0.6906 - accuracy: 0.5678 - val_loss: 0.6884 - val_accuracy: 0.6016\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 805s 3s/step - loss: 0.6850 - accuracy: 0.6073 - val_loss: 0.6831 - val_accuracy: 0.6074\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 253s 843ms/step - loss: 0.6806 - accuracy: 0.6069 - val_loss: 0.6775 - val_accuracy: 0.6426\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 253s 844ms/step - loss: 0.6774 - accuracy: 0.6099 - val_loss: 0.6746 - val_accuracy: 0.5938\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 253s 845ms/step - loss: 0.6751 - accuracy: 0.6015 - val_loss: 0.6692 - val_accuracy: 0.6562\n"
     ]
    }
   ],
   "source": [
    "train_generator = make_generator('train')\n",
    "validation_generator = make_generator('test')\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=final_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_generator,\n",
    "    epochs=5,\n",
    "    steps_per_epoch=300,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=16)\n",
    "\n",
    "model.save('iscute.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 252s 840ms/step - loss: 0.6719 - accuracy: 0.6063 - val_loss: 0.6748 - val_accuracy: 0.5879\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 251s 837ms/step - loss: 0.6695 - accuracy: 0.6126 - val_loss: 0.6724 - val_accuracy: 0.6035\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 252s 840ms/step - loss: 0.6668 - accuracy: 0.6121 - val_loss: 0.6633 - val_accuracy: 0.5996\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 252s 839ms/step - loss: 0.6681 - accuracy: 0.6063 - val_loss: 0.6721 - val_accuracy: 0.6016\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 253s 843ms/step - loss: 0.6640 - accuracy: 0.6138 - val_loss: 0.6548 - val_accuracy: 0.6406\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 252s 840ms/step - loss: 0.6645 - accuracy: 0.6128 - val_loss: 0.6676 - val_accuracy: 0.6113\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 252s 839ms/step - loss: 0.6630 - accuracy: 0.6160 - val_loss: 0.6471 - val_accuracy: 0.6543\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 252s 839ms/step - loss: 0.6616 - accuracy: 0.6129 - val_loss: 0.6515 - val_accuracy: 0.6348\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 252s 840ms/step - loss: 0.6615 - accuracy: 0.6103 - val_loss: 0.6611 - val_accuracy: 0.6172\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 242s 806ms/step - loss: 0.6615 - accuracy: 0.6103 - val_loss: 0.6686 - val_accuracy: 0.5977\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=300,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=16)\n",
    "\n",
    "model.save('iscute2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_loaded, filename):\n",
    "    img = image.load_img(filename, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    images = np.vstack([x])\n",
    "    return model_loaded.predict (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not cute, cute\n",
      "data/predict\\000ada55d36b4bcb.jpg: [[0.15954116 0.84045887]]\n",
      "data/predict\\00a300e8b0cef4d3.jpg: [[0.16837561 0.83162445]]\n",
      "correct\n",
      "data/predict\\00a3654c1cf00d11.jpg: [[0.5946191  0.40538093]]\n",
      "data/predict\\00a36f96e31731c4.jpg: [[0.16574934 0.8342506 ]]\n",
      "data/predict\\00a42a80e5e8d194.jpg: [[0.21026315 0.78973687]]\n",
      "data/predict\\00a7655d4eabf186.jpg: [[0.1970007 0.8029993]]\n",
      "data/predict\\00abfe9035972732.jpg: [[0.23064129 0.7693587 ]]\n",
      "data/predict\\00ac70478d4727bf.jpg: [[0.11923692 0.8807631 ]]\n",
      "correct\n",
      "data/predict\\00acf53b127218c2.jpg: [[0.59964865 0.40035135]]\n",
      "correct\n",
      "data/predict\\01IrQfV.jpg: [[0.30891746 0.6910825 ]]\n",
      "data/predict\\01KyAvz.jpg: [[0.67662096 0.323379  ]]\n",
      "correct\n",
      "data/predict\\01ZSRgh.jpg: [[0.23858742 0.76141256]]\n",
      "correct\n",
      "data/predict\\0thB4ca.jpg: [[0.20742321 0.7925768 ]]\n",
      "correct\n",
      "data/predict\\0TizTVN.jpg: [[0.32586297 0.67413706]]\n",
      "correct\n",
      "data/predict\\0tOXueK.jpg: [[0.42276764 0.57723236]]\n",
      "correct\n",
      "data/predict\\0TskMIN.jpg: [[0.23823695 0.76176304]]\n",
      "correct\n",
      "data/predict\\0udCiHx.jpg: [[0.39574084 0.60425913]]\n",
      "correct\n",
      "data/predict\\0UHSdfR.jpg: [[0.27829146 0.7217086 ]]\n",
      "correct\n",
      "data/predict\\0uhYm1R.jpg: [[0.24272986 0.7572701 ]]\n",
      "correct\n",
      "data/predict\\0vnm9YP.jpg: [[0.17568548 0.82431453]]\n",
      "correct\n",
      "data/predict\\0vo7tgm.jpg: [[0.28346825 0.7165317 ]]\n",
      "correct\n",
      "data/predict\\0vtozcK.jpg: [[0.23627952 0.76372045]]\n",
      "correct\n",
      "data/predict\\0vTyJgU.jpg: [[0.20515807 0.79484195]]\n",
      "correct\n",
      "data/predict\\0VxIKIl.jpg: [[0.13229086 0.86770916]]\n",
      "correct\n",
      "data/predict\\0VzNwOj.jpg: [[0.31749985 0.6825001 ]]\n",
      "correct\n",
      "data/predict\\0W4l8c9.jpg: [[0.24222508 0.75777495]]\n",
      "correct\n",
      "data/predict\\0wUbr21.jpg: [[0.26761168 0.7323883 ]]\n",
      "correct\n",
      "data/predict\\0wz8jsI.jpg: [[0.21206689 0.78793305]]\n",
      "correct\n",
      "data/predict\\0WzVO5n.jpg: [[0.29184297 0.70815706]]\n",
      "correct\n",
      "data/predict\\0xv2U3V.jpg: [[0.4073135 0.5926865]]\n",
      "correct\n",
      "data/predict\\0y4QiSC.jpg: [[0.26334476 0.73665524]]\n",
      "data/predict\\0YAe6Qx.jpg: [[0.5235938  0.47640622]]\n",
      "correct\n",
      "data/predict\\0YkhkYR.jpg: [[0.49991375 0.5000862 ]]\n",
      "correct\n",
      "data/predict\\0YL53mj.jpg: [[0.48723102 0.51276904]]\n",
      "correct\n",
      "data/predict\\0YpzRjL.jpg: [[0.15179133 0.84820867]]\n",
      "correct\n",
      "data/predict\\0z15oHd.jpg: [[0.1577969 0.8422031]]\n",
      "correct\n",
      "data/predict\\0zcgOHq.jpg: [[0.40406436 0.59593564]]\n",
      "correct\n",
      "data/predict\\1AAw2sI.jpg: [[0.22666132 0.7733387 ]]\n",
      "correct\n",
      "data/predict\\1AZ5pLq.jpg: [[0.46156877 0.5384312 ]]\n",
      "correct\n",
      "data/predict\\1azQjo3.jpg: [[0.39501944 0.6049806 ]]\n",
      "data/predict\\SPOILER_cute_spiderbart.png: [[0.35341036 0.64658964]]\n",
      "not cute, cute\n",
      "correct: 31 (0.775%)\n",
      "FP: 7, FN: 2\n"
     ]
    }
   ],
   "source": [
    "model_loaded = load_model('iscute2.h5')\n",
    "model_loaded.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "correct = 0\n",
    "falsePositives = 0\n",
    "falseNegatives = 0\n",
    "total = 0\n",
    "print('not cute, cute')\n",
    "for filename in glob.glob('data/predict/*'):\n",
    "    classes = predict(model_loaded, filename)\n",
    "    if (len(filename) == 33):\n",
    "        total += 1\n",
    "        if (classes[0][0] > classes[0][1]):\n",
    "            correct += 1\n",
    "            print('correct')\n",
    "        else:\n",
    "            falsePositives += 1\n",
    "    elif(len(filename) == 24):\n",
    "        total += 1\n",
    "        if (classes[0][0] < classes[0][1]):\n",
    "            correct += 1\n",
    "            print('correct')\n",
    "        else:\n",
    "            falseNegatives += 1\n",
    "    print(filename + ': ' + str(classes))\n",
    "\n",
    "print('not cute, cute')\n",
    "print('correct: ' + str(correct) + ' (' + str(correct / total) + '%)')\n",
    "print('FP: ' + str(falsePositives) + ', FN: ' + str(falseNegatives))"
   ]
  }
 ]
}